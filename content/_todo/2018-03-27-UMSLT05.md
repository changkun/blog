---
title: 'UMSLT05: The Consistency of Learning Process'
tags:
  - 统计学习
  - 理论
  - 数学
  - 泛函分析
  - 统计学
  - 概率
  - 机器学习
  - 深度学习
  - 神经网络
date: 2018-03-27 19:41:16
id:
---

上一篇文章中我们聊到了保证学习过程是一致的两种原则，一种是随机逼近原则，另一种是风险最小化原则。『一致』是一个纯数学概念，那么在定义学习问题的过程中，我们怎么定义学习过程的一致性呢？

## 学习一致性的定义

**定义 1**：假设 $Q(z, \alpha_m)$ 使得在独立同分布样本 $z_1, …, z_m$ 上的经验风险泛函 $R_{emp}=\frac{1}{m}\sum_{i=1}^{m}{Q(z_i,\alpha)}$ 最小的函数。如果序列 $R(\alpha_i)$ 和 $R_{emp}(\alpha_i), i=1,2,...$均依概率 $P$ 收敛于同一个极限 $\inf_{\alpha\in\Lambda}{R(\alpha)}$，则称 ERM 原则对于函数集 $Q(z, \alpha), \alpha\in\Lambda$ 和真实的概率分布函数 $F(z)$ 是一致的。

从直观理解上看 ERM 方法说的是这样一件事情，学习过程的一致性等价于：期望风险和经验风险都随着样本的增加而收敛到最小可能的风险值。换句话说，如果一个 ERM 方法能够提供一个函数序列 $Q(z, \alpha_i), i=1,2,…$ 使得这个序列的期望风险和经验风险都收敛到最小可能的风险值，那么我们就说这个 ERM 方法是一致的。$R(\alpha_i)$ 的收敛性保证了所达到的风险收敛于最好的可能值，而 $R_{emp}(\alpha_i)$ 保证了可以在经验风险的取值基础上估计最小可能的风险。

可惜的是，对于上面这种传统的一致性的定义，得到一致性的充分必要条件是不可能的，因为这种定义中包含了一种叫做**平凡一致**的情形。

我们反面考虑这个问题：假设我们已经建立了某个函数集 $Q(z,\alpha), \alpha\in\Lambda$，对这个函数集 ERM方法是不一致的。那么考虑这个函数集的一个扩展集，这个集包含了前面的函数集和一个额外的函数 $\phi(z)$。若这个额外函数满足 $\forall z, \inf_{\alpha\in\Lambda}Q(z,\alpha)>\phi(z)$，即 $\phi(z)$ 是函数集 $Q(z,\alpha), \alpha\in\Lambda$ 的一个下界。对于任何分布函数和任意数量的观测，经验风险的最小值都在函数 $\phi(z)$ 上取得，并同时给出了期望风险的最小值。这时这个扩展的函数集合对于 ERM 方法而言是一致的。

上面的例子说明，平凡一致是存在的，**平凡一致的充分必要条件就是一个函数集中是否包含一个最小化的函数**。因此任何采用传统定义的一致性理论必须能够确定其中是否有平凡一致的情况，即**一致性理论必须考虑到给定函数集中特定的函数，这是我们不希望看到的**。

我们希望能够建立一种 ERM 方法的一致性理论，使其不依赖于函数集中个别元素的特性，而是依赖于函数集的整体特性（也就是我们所说的模型容量）。为此，我们需要修正一下一致性的行医，从中剔除平凡一致的情况。

**定义 2**：对于函数集 $Q(z, \alpha), \alpha\in\Lambda$，定义其子集 $\Lambda(c) = \{\alpha: R(\alpha) = \int Q(z, \alpha) dF(z) > c, \alpha\in\Lambda\}$ 如果对函数集的任意非空子集 $\Lambda(c), c\in(-\infty, \infty)$ 都有 $\inf_{\alpha\in\Lambda(c)}{R_{emp}(\alpha)} \xrightarrow[m\rightarrow\infty]{P} \inf_{\alpha\in\Lambda(c)}{R(\alpha)}$