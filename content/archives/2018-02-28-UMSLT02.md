---
date: 2018-02-28 18:45:36
id: 246
path: source/_posts/2018-02-28-UMSLT02.md
tags:
  - 统计学习
  - 理论
  - 数学
  - 泛函分析
  - 统计学
  - 概率
  - 机器学习
  - 深度学习
  - 神经网络
title: 'UMSLT02: A Breif History of Neural Networks'
---

上一篇文章中我们从第一个学习模型感知器的提出聊到了人们在研究学习机器的过程中发现因果问题的反演是一个 ill-posed 的问题，正则化方法在解决 ill-posed 问题的过程中扮演了重要的角色。ERM 作为应用学派归纳推理原理的一种不证自明的方法，这篇文章我们就这个话题继续聊下去。

值得一提的是，从一个范围很宽的密度集合中估计密度函数就是一个 ill-posed 的问题。早期的统计学发展了一种叫做参数统计的方法，其最大似然估计，作为一种不证自明的方法来对有限个参数决定的密度集合进行估计。但如果我们连数据的分布假设都不敢假设，换句话说——对一种未知（参数未知）的分布进行估计，最大似然估计就无能为力了。这时候人们发展了很多非参数方法，从而建立了现代庞大且意义非凡的非参数统计学。值得一提的是，这些方法的基本假设是使用大量样本来估计密度，他们可以克服 ill-posed 的问题。

<!--more-->

## 归纳推理与随机的本质

上个世纪的科学家们在思考实现智能这一问题上产生了不同的分歧。其中以冯诺依曼为首的学派认为作为人类自身我们并不知道智能产生的原因，但是我们可以观察智能导致的结果以及反映（图灵测试），进而发展出了现代的计算机科学。以香农为首的学派认为智能的本质是信息的传递，我们需要对信息的载体进行某种度量，最终发展出了信息论。

如果我们在思考智能，我们不得不问自己，究竟什么才是智能？人在产生智能的过程中，在脑中实施的归纳以及推理的本质是什么？现实世界中的随机现象产生的本质究竟是什么？这两个问题首次由 Solomonoff、Kolmogorov 和 Chaitin 给出了他们的答案。对于随机的本质，从信息论的角度来说，**如果我们有一个长度为 l 的信息串，如果不存在任何复杂度远小于 l 的算法能够产生出这个信息串，那么这样的串就是一个随机串。**请注意这里的概念，「不存在」「远小于」「产生」。

后来，人们在这个思想的基础上，提出了学习问题的最小描述长度（MDL）归纳推理[Rissanen, 1978]，并认为产生只能的本质应该是在思考的过程中对描述长度的最小化。这样的一种归纳推理跟我们之前提到的 ERM 承载着完全不一样的归纳原则 [Vapnik et al. 1978]。

## 二十世纪的神经网络研究

很多人都误以为 Jeffry Hinton 是反向传播的发明人，但实际上他只是将这个方法加以推广之人。如果我们要追溯反向传播的历史，就会发现反向传播其实并不是在研究学习机器中发现的。当时的人们在解决一些控制问题是，希望研究如何进行最优误差传播时，将链式法则应用了起来，最后才在感知器中被「重新发现」[LeCun, 1986]、[Rumelhart et al. 1986]。但是，基于梯度的方法似乎只能保证找到局部极小值或者鞍点（注意，直到现在仍然是这样）。反向传播的出现提起了人们第二次对感知器的极大兴趣。以至于后来人们将感知器的名字更名为了神经网络，并将生物学家也拉来入伙。但是大量的理论学家普遍认为研究人是如何学习是没有意义的，就像当年人类研究鸟类如何飞行与制造飞机并没有太多实质性的帮助，反倒是人们锲而不舍的研究动力学以及流体力学等学科发展的夯实的理论基础才最终得以建造飞机。那个时候基于神经网络的学习理论并没有带来多少实质性的进展，在实践中观察到的大量过拟合现象，其实都是 ill-posed 问题中的 false structure 现象。

这样看来，如今深度学习的发展也是完全类似，从将神经网络重新命名为深度学习，从以理论为基础的算法研究到以模型架构和实验为基础的实证研究，似乎是在暗示着我们下一个神经网络的「寒冬」。

## 非正式个人评述

本节我们从非参数统计的建立背景谈起，聊到了归纳统计以及随机性的本质，并回顾了上个世纪八十年代神经网络的研究背景。通过回顾上个世纪神经网络理论的研究历史，我们可以初步的窥探如今的深度学习的广泛发展，是由于计算量的提升、数据量的提升、非凸优化算法的发展、网络结构的改变带来的，大部分传统理论其实对如今的一些深度学习现象已经做过浅层网络的研究，但是从 2006 年至今差不多十余年的深度学习研究，还是没有从本质上推进对学习过程本质的认识（VC 维理论为什么失效），尚未出现任何非常靠谱的理论。这样来看，我们应该追根溯源，从最一般性的结论出发，看看在学习理论的发展过程中，确定各种理论的成立条件，找出究竟是哪一个环节出了问题。

本文未解释的部分：

- 为什么密度估计问题是一个 ill-posed 问题？
- 最大似然估计在参数统计中是怎么做到「不证自明」归纳的？
- 最小描述长度归纳推理是怎样进行归纳的？与 ERM 的不同之处在哪儿？
- 反向传播在多层感知器中扮演着怎样的角色？
- 过拟合现象与 ill-posed 问题的区别是什么？
- ill-posed 问题中的 false structure 现象是什么？
- 上个世纪神经网络的「寒冬」是什么？

我们在随后的系列文章中在继续对这些问题进行讨论。

## 进一步阅读的参考文献

- [Rissanen, 1978] Modeling by shortest data description
- [Vapnik et al. 1978] Nonparametric methods for estimating probability densities
- [LeCun, 1986] Learning processes in an asymmetric threshold network
- [Rumelhart et al. 1986] Learning internal representations by error propagation