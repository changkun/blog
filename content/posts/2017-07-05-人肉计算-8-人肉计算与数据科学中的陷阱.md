---
date: "2017-07-05 15:21:51"
toc: true
id: 234
slug: /posts/human-computation-8
aliases:
    - /archives/2017/07/234/
tags:
    - 人肉计算
    - 数据科学
    - 道德陷阱
title: '人肉计算(8): 人肉计算与数据科学中的陷阱'
---

这个人肉计算的系列主题差不多快要接近尾声了。在之前的几篇文章里我们已经看了足够多的输入数据聚合的例子，了解到了一些数据处理的方法。是时候从伦理道德的角度来讨论人肉计算领域存在的问题了。

<!--more-->

## 人肉计算的陷阱

### 隐私风险

人类的计算可能会损害这两个方面的利益：

- 个人隐私
- 群体隐私：针对集体成员的愿望披露社会团体（如民族，宗教，性取向团体）的行为（如健康，离婚率，教育能力等）

一个容易被察觉到的问题就是：**一个社会在某些情况下要求披露群体行为的权利是道德的吗？**

### 数据操作风险

数据操作也具有风险。举例来说，用户的打分有助于评估亚马逊的产品评级[^1]：「评价者给予折扣或免费产品奖励更高的评级，有害地影响亚马逊的评分系统。」 

[^1]: Samuel Gibbs: “Incentivised reviews are warping Amazon’s product star ratings, report says”, The Guardian, 20 September 2016

### 给予、获取还是「暗抢」（Hidden Robbery）？

**对人肉计算系统做贡献的这么多人当中，谁才是知识产权人？**

这个问题是很难回答的，因为所有权很少留给贡献者：

- 聚合数据可能需要保持来源（例如科学）的贡献。
- 事后的撤回可能是不道德的（例如在医学研究中或者可能导致破产）。

**人类对HC系统的贡献的显著惩罚是什么？**

**人类贡献者是否有意识地自动提供或交换其数据或知识？**

### 帮助还是控制？

在FeedVis项目中，Karrie Karahalios 和她的团队向 Facebook 用户在左边显示了他们的朋友的所有帖子，并在右边显示他们的新闻源。

参与者对于他们左边还有多少个帖子感到好奇和惊讶，有些人则对 Facebook 将其家人和亲人的帖子隐藏感到失望。62.5％的人没有意识到或不确定 Facebook 是否过滤掉了朋友的帖子[^2][^3]。

[^2]: Christian Sandvig, Karrie Karahalios, Cedric Langbort: Uncovering Algorithms: Looking inside the Facebook News, Conversation at the Berkman Center for Internet & Society at Harvard University, 22 July 2014 https://cyber.harvard. edu/events/luncheon/2014/07/sandvigkarahalios
[^3]: Motahhare Eslami, Amirhossein Aleyasen, Karrie Karahalios, Kevin Hamilton, and Christian Sandvig: “FeedVis: A Path for Exploring News Feed Curation Algorithms”, Proc. 18th ACM Conf. Companion on Computer Supported Cooperative Work & Social Computing (CSCW), pages 65-68, 2015

- 「我们的工作，我们的目标是帮助人们看到对他们最有意义和有趣的内容。」
- 20年前，「你所有的消息都通过电视和报纸过滤了」
- 「这并不是在 Facebook 上的信息不够多样化……而是我们还没有吸引更多比例的人」

马克·扎克伯格（Mark Zuckerberg）拒绝承认 Facebook 上的假新闻影响了201年11月美国总统选举的结果[^4][^5]。

[^4]: Mark Zuckerberg Live from the Techonomy Conference, 10 November 2016 https://youtu.be/fxiCbvgzYag
[^5]: Olivia Solon” “Facebook’s fake news: Mark Zuckerberg rejects ‘crazy idea’ that it swayed voters”, The Guardian, 11 November 2016

扎克伯格与传统媒体的比较令人信服吗？ 

扎克伯格对 Facebook 新闻过滤的辩护令人信服吗？

免费服务是被认为是操纵的「算法改进」的借口吗？

以下的条件是否能避免操纵数据？

- **透明**（transparency）：披露诸如新闻选择器和激励计划之类的算法，以及它们的目的、为什么是有潜力的等等。
- **共同目标**（shared goals）：人类计算系统应该对它的贡献者共享其目标（例如学习系统）？
- **满足期望**（fulfilled expectations）：数字社交媒体应该例如不修改谁收到什么消息或帖子的时间和方式 - 过滤泡沫和超越。

### 科学实验的伦理问题

许多人类计算系统正在不断地在用户身上做实验。

- 这样的实验在伦理上是否可以接受？
- 提供新的或改进的援助服务于一些用户而不是不是给别人是否可以接受？
- 用户很少意识到他们被用于实验是可以接受的吗？
- 「道德委员会」是否会为人类计算系统提供监督？ 如果是，董事会成员应如何提名，董事会成员应由谁负责？
- 立法机关对人类计算系统是否承担「伦理委员会」的角色？

##  数据驱动方法的陷阱

数据科学技术的计算分析在于：

1. 数据搜集
2. 数据清洗
3. 为感兴趣的变量建立数学关系的模型
4. 用收集和策划的数据对模型进行训练
5. 通过将经过训练的模型应用于新数据来推断感兴趣变量的关系
6. 检查推断关系的有效性
7. 如果有效性检查不是决定性的，则改进模型训练数据并重复该过程

最后两个步骤往往忽视了可能产生的两个伦理道德问题：

- 证据（收集的数据）和理性（数学模型）是否被错误的提出？
- 使用数学、统计和技术进行操作是否属于不择手段？

具有人肉计算数据的数据科学（特别是监督机器学习）通常用于：

- 预测消息是否可能是垃圾邮件（所谓的「垃圾邮件检测」，一个有问题的面值）
- 预测候选人的工作或课程是否可能方便
- 预测读者可能点击的广告
- 预测信用卡交易是否可能是欺诈性的
- 预测旅行者或社区成员是否可能是恐怖分子
- 预测对手的行为，选择运动中的策略
- 预测体育比赛的结果

基于数据的方法的陷阱：

- 过拟合
  + 该模型在训练数据上表现良好，但新数据表现不佳。
- 不合适
  + 该模型对训练数据进行了较为深入的研究。
  + 通过使用不同的数据来训练和测试模型，可以检测到过度拟合，通常通过将两个部分分为两部分：收集的数据，一部分用于训练，另一部分用于测试。
- 测试陷阱
  + 训练和测试数据中不相关的模式示例：随着时间推移用户活动的数据，尝试和测试参考相同用户的数据。经过训练的模型可以检测用户而不是用户活动模式。
  + 在测试数据中选择一个最佳性能的模型之一是使用测试数据作为训练数据（用于选择模型）。 在这种情况下，需要第三个数据集来测试所选择的模型。

## 具有人肉计算数据的数据驱动方法的陷阱

### 知识产权妨碍数据集

使用数据来训练算法，反对其知识产权所有者的意愿而不是引用。从选择的书籍中，神经网络「学习」产生自然语音句子[^6][^7]。

[^6]: Samuel R. Bowman and Luke Vilnis: “Generating Sentences from a Continuous Space”, Arxiv:1511.06349v4, 1016
[^7]: Richard Lea: “Google swallows 11,000 novels to improve AI’s conversation”, The Guardian, 28 September 2016

### 使用考虑的滥用

使用数据来训练算法，数据贡献者不同意或不了解，同时对数据做出贡献 [^8]

- 建设战争软件
- 不包括某些预期表现不佳的学生
- 不雇用某些预期至关重要或受到疾病影响的人
- 分析社会媒体上发现车主人格特征（如认真，组织良好）的岗位，并在检测到特征后设定汽车保险价格

[^8]: Graham Ruddick: “Admiral to price car insurance based on Facebook posts”, The Guardian, 2 November 2016

建立性工具，利用人机计算训练集开发监督机器学习。事实上，性取决于一个人的心理学，而这又依赖于“经验模式”。一个应用程序已经显示每分钟发送一个振动器的温度和振动强度和模式，每次他们改变到设备的生产者[^9]。

[^9]: goldfisk and follower: “Breaking the Internet of Vibrating Things”, DEF CON 24 Hacking Conference 2016

「Walmart.com使用的是不同于其他大型在线供应商的交易系统来创建交叉销售链接。 例如，Amazon.com基于购物者之前购买的消费者以及购买某个商品的其他消费者购买的建议。Walmart.com手动将电影分配给特定的『项目展示组』，如科幻小说或非洲裔美国文化。 该公司内部开发的软件然后生成指导购物者到该组中的其他电影的链接。 除了“猿”盒装，King包可能已经链接到随机选择的其他盒装套装[^10]」。

[^10]: Wal-Mart blames human error for o↵ensive link – Attempt to promote Martin Luther King Jr. film went horribly awary, Techand Gadgets on NBCNEWS.com went horribly awary, 6January 2006

### 有偏差的数据集

- Jacky Alcine 观察到 Google Photos 将他和一位朋友标记为大猩猩。 2015年6月28日 https://twitter.com/jackyalcine/status/615329515909156865?lang=de
- Madikizela Malema 观察到，谷歌答复「不专业发型」返回黑人妇女的图片，为白人妇女的「专业发型」。 2016年4月5日 https://twitter.com/jackyalcine/status/?615329515909156865?lang=de
- Kabir Alli观察到，「三名黑人青少年」的 Google 搜索结果是警察拍照，「三名白人青少年」的搜索结果是微笑的人的照片。 7. Juni 2016 https://twitter.com/ibekabir/status/740005897930452992

由人类产生的学习算法的训练数据可能反映了这些人（有意识或无意识）的偏见。
没有以下两个步骤（见上文）：

- 检查推断的关系的有效性
- 如果有效性检查不是决定性的，则进行精炼

模型和训练数据并重复该过程。 监督机器学习可以「编码」人的偏见。

在某些情况下，前面提到的两个步骤不是或几乎是不可能的，也可能是不可能的，但不能执行（以减少花费）：

- 选择求职者
- 选择学生
- 选择贷款申请人

这将导致**自我实现的预测因子**（self-fulfilling predictors）。

### 算法偏差的增强

继承社会对训练数据的偏见的模式可以加强社会的偏见。

然而，反映社会偏见的算法使其有机会克服其偏见，只要它们不是自我实现的预测因子。

### 算法触发的社会一致性

更一般地，使用人类计算训练集的监督机器学习可以导致社会一致性（例如选择要支持的研究）。

然而，如果不是自我实现的预测因子，反映社会社会符合性的算法就有机会克服这种一致性。

#### 思考（虚构的例子）

考虑以下研究资助计划：12 个「六年初级教授国家补助金」每年颁发给全国最多150 名潜在申请人的科学优势博士后。

一位大学校长发表了它对过去三次研究计划拨款的的观察结果：
- 在过去三年的资助计划中，资助在大学中相对均匀分配 —— 为什么？
- 在过去三年的资助计划中，成功申请人的研究领域相对均匀分布 —— 为什么？
- 大学校长支持的申请越少，则这些申请的成功越有可能 —— 为什么？

大学校长从她所支持的大学中选择少量申请，以优化大学的「赠款收入」 —— 这不够公正吗？后来，她雇用数据科学家，根据以下参数建立「授权申请选择模式」：

- 申请人的人口统计学（性别，年龄等）
- 过去的赠款
- 目前的「主流」研究领域

- 从数据科学模式中寻求援助和公正是不公平的吗？

所使用的模式，大学获得「赠款收入」的公平份额 —— 它不会验证数据科学模型吗？上述情况有问题，原因有几个：

- 根据科学价值提供赠款的说法。 我是大学校长支持的事实
  应用程序起作用。 事实上，没有涉及几位科学家的正式选拔过程，没有科学家，所以没有大学校长可以正确选择最好的应用
- 双重选择过程的透明度，一个选择甚至从申请人未知
- 希望尽可能优化基于数据科学模型的大学成功
- 涉及的数量很少（12个赠款，150个申请人，3年的观察）使得有意义的分析不可能
- 大学「补助金收入」的短期优化不断地使大学的重点受到伤害
- 不可能检查模型的有效性，使其成为一个自我实现的预测因子

场景的负面后果：

- 不可预测性促进煽动 —— 不公平
- 在大学中不断改变焦点，防止建立能力 —— 短视
- 数学模型及其编码增加了甄选过程的透明度 —— 愚蠢的专业知识

情景还有不良后果吗？大学校长雇用的数据科学家可以做些什么来建立「授权申请选择模式」？如果不是选择申请人，该方法用于决定

- 如果平民被视为恐怖分子
- 如果穿制服的小孩是威胁的话

### 大众失业

如果机器人可以训练模仿人类工作者，就有大规模失业的风险引发大规模的贫困[^11][^12]：呼叫中心工作人员、销售、教师、医务人员、工作人员、仓库工人、司机等等。

[^11]: Erik Brynjolfson, Andrew McAfee: Race Againt the Machine, Digital Frontier Press, 2011
[^12]: Martin Ford: The Rise of the Robots, Oneworld Publications, 2015

国家对所有人的基本生活必需品（通常称为「普遍基本收入」）将提供解决人类软件学习造成的群众失业问题的解决方案？谁属于「全部」？

## 市场反身性的恶性循环

回忆一下我们在第一篇文章中谈到的「反身性」：市场可以体验到反刍性的恶性循环，即市场泡沫（从17世纪荷兰的郁金香疯狂泡沫到2000-2001年的Dot-com泡沫，到2007-2008年的次贷抵押泡沫）：

1. 泡沫之前：有些价格持续上涨的时间比平时要长。
2. “繁荣”：越来越多的交易者被意想不到的收益引发，因为他们希望稍后以更高的价格出售，因此放松了风险和购买意愿，从而有助于保持价格上涨。
3. “破产”：在某种程度上，足够的交易者来到理性，停止购买使价格飙升而大幅度失败。

## 人肉计算市场：是一种新的开拓形式吗？

- **开放经济**（Access Economy）：基于对商品和服务的所有权进行或出租的交易的经济。
- **千禧经济**（Gig Economy）：以经济为导向，以就业为导向，即以临时就业为基础的经济。

巨人经济是否导致工人的独立或独立幻想？（土耳其和 Uber 司机是否需要依赖）？

## 意图博弈与游戏化：「面包与马戏」再续前缘？

完全「GWAP」或游戏化工作场所对人类有负面影响吗？可能的风险：

- 智力衰弱
- 游戏成瘾

## 人肉计算和数据分析

给予机器所有权利？给予技术专家所有权利？进一步阅读：[^13][^14][^15][^16]

[^13]: Olivia Solon: “Amazon pushes customers towards pricier products, report claims”, The Guardian, 21 September 2016
[^14]: Cathy O’Neil: “How algorithms rule our working lives”, The Guardian, 1 September 2016
[^15]: John Naughton: “Machine learning: why we mustn’t be slaves to the algorithm”, The Guardian, 16 October 2016
[^16]: Cathy O’Neil: “Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy”, Allen Lane, 2016