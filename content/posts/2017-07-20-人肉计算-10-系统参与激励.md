---
date: 2017-07-20 07:54:29
id: 237
path: source/_posts/2017-07-20-人肉计算-10-系统参与激励.md
tags:
  - 人肉计算
title: '人肉计算(10): 系统参与激励'
---

这是「人肉计算」系列文章的最后一篇。我们来讨论参与激励的相关问题。

<!--more-->

> 本文尚未仔细整理

## 冷启动、噪声、可持续发展、参与幂律

像ARTigo和duolingo这样的系统无法启动“冷”意味着两者

- 足够的用户（玩家或学习者），
- 足够的用户生成内容

这些平台需要为用户提供令人满意的（玩或学习）体验。
只要用户数量或数据量不足，雇用人们就可以避免这种系统的冷启动。
雇用工人的另一种办法是建立一个自给自足的（或自给自足的）生态系统。

人类计算系统，如reCAPTCHA，ARTigo和duolingo可以收集很多“噪音”，即质量不足的数据。

reCAPTCHA，ARTigo和duolingo的工作不会因为反复的人类评估，集体协议而收集到的噪音而受到损害。

必须实验性地估计重复人类评估的数量和丢弃噪声所需的集体协商水平。

只要足够的数字化系统将其与要识别的单词进行交流，reCATCHCHA就可以发挥作用。
ARTigo需要大量的艺术品图片才能提供良好的播放体验。
杜兰戈可以发挥作用

- 有人希望学习语言，
- 可以准备以这些语言翻译的文本，即分配给适当的学习单位。

如果没有人力收集艺术品图像和准备文字翻译，ARTigo和duolingo是不可持续的。

挑战是设计人类计算系统收集工作，确保人类计算系统（如ARTigo和duolingo）的可持续性。

使用诸如ARTigo和duolingo等人类计算系统，少数用户生产大部分数据。

以递减的顺序绘制人类计算系统（许多用户）的用户的表现通常产生幂律。

大约80％的效果（y轴值）来自原因的20％（x轴值）。 实际上，这个比例可能偏离80-20。

所谓的长尾巴（黄色区域）可以非常有效：

- 每个用户从长尾不产生多少，
- 但他们的数量是非常大的。

有效的人力计算系统通常可以通过确保为高度活跃和临时的用户提供良好的用户体验来实现。 请注意，高活跃用户通常开始是临时用户。

## 参与原因

内在动机：

- 对维权的兴趣
- 公民科学项目
- 社会参与和渴望帮助
- 有兴趣成为“集体智慧”的一部分

内在动机甚至在众包市场上起作用：

- 付款不是工人在AMT上选择某些任务的主要原因（Ipeirotis 2010）。

外在动机：
- 付款
- 社会动机：
    + 注意
    + 声誉
    + 状态
    + 作为社区的一部分
- 娱乐：游戏
    + 与网站义务的互动：
    + 工作
    + 学习

## 参与条件

- 公开
- 明确的目的
- 可用性
- 相干性
- 充足的奖励
- 透明度和公平信任与伦理

宣传是人类计算系统参与的第一个条件：
   许多系统竞争人们的注意力和/或参与。
人类计算系统可以被知道
   定期报告系统的成就，
   进一步发展，
   创造一种社区归属感
系统的贡献者，
   在系统（潜在）用户使用的媒体上公布系统。

通常宣传人类计算系统
   是一个缓慢的过程，
   需要很多努力。

清楚的奖励，目的和所有权：
   系统向其贡献者提供什么（有形或无形的）奖励，
   系统的目的是什么，也就是说，
   收集的数据用于，
   谁从系统成果中受益，
   谁拥有
   系统收集或生成的数据，
   知识产权（知识产权）对系统的成果。
应该向系统的潜在用户明确。

可用性
参与的条件是系统设计，特别是用户界面
   使其简单直观
   使用系统，
   要学习如何更好地使用系统，
   给系统低入门和重入门槛。
该系统还应提供其使用所需的工具（如在众包市场上进行任务搜索）
   AMT上列出的任务如何显着影响哪些任务被选择（Horton和Chilton 2010）

系统目的的一致性，特别是没有冲突的目的：
   如果承诺，贡献者不应该面对艰苦的工作，
   资助创意项目的利润来源的融资不太可能排除在资助企业（Kickstarter的选择）中提供股份，
   创新平台不能给知识产权（知识产权）违法的机会，
   学习平台应该有最先进的学习资料。
   等等

充足的奖励
   游戏必须玩得开心
   付款必须足够。
   学习提议必须承诺给学习者。
   公民科学平台必须将其参与者通知使用平台所取得的进展。
   创新平台必须发布足够的解决问题。
   众包渠道必须发布足够的资金项目。
   等等

“贡献经济”要求，像任何经济，透明度，控制和信任：
   宣布社会参与或创新等目标的人类计算系统必须证明其努力实现这些目标。
   通常需要控制由贡献者本身和/或运行系统的实例的努力。
   贡献越重要，预期控制越多，反之亦然：
   从事的维基人士不断控制他们大大贡献的文章。
   面临不正当拒绝工作的工人不能提供高质量和/或复杂的工作。
信任来自
   透明度，
   控制，
   道德管理 - 见下文“伦理”。

## 恶意参与检测

众包系统吸引垃圾邮件发送者，
   作弊贡献者
可以通过以下方式检测到它们的行为：
   不时评估对所谓的黄金标准的贡献，即系统所构成问题的已知解决方案，
   检查贡献者IP地址以检测恶意多重贡献，
   黑名单恶意贡献者。

## 激励

了解贡献者
   适当的奖励
  游戏化
   社会维度，地位和声誉激励的游戏理论

激励需要知道贡献者：
   贡献者及其期望可能会随时间而变化。 因此
   用户型号，
   奖励和奖励
可能需要实现。

适当的奖励是激励的一部分：
   适当的奖励是公平的报酬。
   为了保持适当，可能需要调整奖励（例如，当贡献者的技能增加时）。
   奖励的适当性可能取决于竞争系统给予的奖励。

游戏化
奖励，通常是为了取得成就而颁发的徽章，是激励的常见方法。
游戏化可能具有（不希望有的）副作用：
   已经观察到StackOverflow用户在获得徽章所需的贡献级别（Anderson等人，2013）时增加了他们的努力水平。
   “在SETI @ home，一个实验性的Zooniverse项目，中断接收徽章意味着人们很可能离开而不是有动力继续分类。”（Lintoot和Reed，2013）

社会维度，状态和声誉。
人类计算系统的贡献者往往会重视从参与到系统中的社区的归属感。
因此，系统可以通过向他们通知他们来激励其贡献者
   地位，即他们在社区内的地位，
   声誉，就是他们的贡献如何受到社会的重视。

激励博弈理论
人类计算系统的游戏理论模型描述了对系统的贡献：
   用户选择动作，
   与“成本”相关的行为（通常表示为“
执行行动所需的时间或精力），行动得到回报。
人类计算系统的游戏理论模型的纳什均衡可以被看作是对系统的理性贡献者的选择。

回想：纳什均衡：
   每个玩家都知道其他玩家的策略。
   没有玩家可以增加一个改变的策略。

使用游戏理论激励参与人类计算系统包括：
1.确定系统潜在贡献者的成本和回报。
2.指定反映这些成本和奖励的系统的游戏理论模型。
3.确定该模型的纳什均衡。
4.估计与这些相关的捐款的质量
纳什均衡
5.优化成本，奖励和模式，以优化与纳什均衡相关的贡献。
6.根据其成本，奖励和模型的细化来优化人类计算系统。
只要需要优化，重复该过程。

可以质疑这种做法：
   球员/贡献者是否合理？
   玩家/贡献者是否理解如何优化奖励，就像游戏理论所假定的那样？
   纳什均衡假设每个玩家/贡献者都知道其他玩家/贡献者的策略。 这是一个合理的假设吗？

球员/贡献者是否合理？
当然不是！ 参见“社会行为分析”一章的“行为经济学”和“行为博弈论”一节。
使用行为博弈理论而不是标准游戏理论的技术，可以说明贡献者的“可预测的非理性”（使用Dan Ariely创造的术语）。

玩家/贡献者是否了解如何优化奖励？
在许多情况下，假设这是一段时间后的情况是合理的。

纳什均衡假设每个玩家/贡献者都知道其他玩家/贡献者的策略。 这是一个合理的假设吗？
在许多情况下肯定不会。
可以使用贝叶斯纳什均衡来代替纳什均衡，以放宽“全面知识”假设。

贝叶斯或不完整的信息游戏：玩家不完全知道游戏的收益。
假设：
   每个玩家都有一些私人信息（游戏收益）。
   玩家的类型{t1，...，tn}的有限集合规定玩家拥有的信息（游戏的收益）。
   有一个先验概率分布Pr在类型上是
   所有玩家都是一样的（“自然”除外），见下文）
   所有玩家都知道（这是“先验知识”）。
在贝叶斯游戏中，至少一个玩家不确定至少另一个玩家的类型，因此是增益功能。

“自然”是贝叶斯游戏中的玩家
   根据以前的每个玩家随机选择一个类型
概率分布Pr类型，
一个策略被定义为将玩家的类型映射到任何一个的函数
的可能的球员的行动（在比赛期间）：

TI
si（ti）
s =（s1（t1），...，sn（tn））
玩家类型我的战略我的策略配置文件

如果没有玩家可以通过成为唯一一个改变策略来提高她的预期收益，那么战略简介就是贝叶斯纳什均衡。



## 伦理

道德是两者
   一个价值本身
   参与的条件。

[^1][^2][^3][^4][^5][^6][^7][^8][^9][^10][^11][^12][^13][^14][^15][^16][^17][^18]
[^19][^20][^21][^22][^23][^24][^25][^26][^27][^28][^29][^30][^31][^32][^33][^34]

[^1]: H. Chi, Niki Kittur, Bryan Pendleton, and Bongwon Suh:“Long tail of participation in Wikipedia”, Augmented Social Cognition Blog from PARC, 15 May 2005
[^2]: Chris Anderson: “The long tail: Why the future of business is selling less of more”, Hyperion, 2006.
[^3]: Chris Anderson: Blog “The long tail”, 2004–2009, http://longtail.typepad.com/the_long_tail/

[^4]: C. Shirky: “Here Comes Everybody: The Power of Organizing Without Organizations”, Penguin Press, 2008
[^5]: A. Forte and A. Bruckman: “Why Do People Write for Wikipedia? Incentives to Contribute to Open-Content Publishing”, Proc. 41st Annual Hawaii International Conference on System Sciences (HICSS), 2008
[^6]: T. W. Malone, R. Laubacher, and C. Dellarocas: “Harnessing Crowds: Mapping the Genome of Collective Intelligence”, Working Paper No. 2009-001, MIT Center for Collective Intelligence, 2009
[^7]: R. Holley: “Crowdsourcing and Social Engagement: Potential, Power and Freedom for Libraries and Users, keynote address to the Pacific Rim Digital Library Alliance (PRDLA) Annual Meeting, 2009
[^8]: M. J. Raddick, G. Bracey, P. L. Gay, C. J. Lintott, P. Murray, K. Schawinski, A. S. Szalay, and J. Vandenberg: “Galaxy Zoo: Exploring the Motivations of Citizen Science Volunteers”, Astronomy Education Review, 2010, AER, 9, 2010
[^9]: P. G. Ipeirotis: “Analyzing the Amazon Mechanical Turk Marketplace”, XRDS: Crossroads, 17, pages 16–21, 2010
[^10]: J. T. Reed, R. Cook , M. J. Raddick, K. Carney, and C. Lintott: “Participating in Online Citizen Science: Motivations as the Basis for User Types and Trajectories”, Handbook of Human Computation, Springer, pages 695-702, 2013
[^11]: J. Chamberlain, U. Kruschwitz , and M. Poesio: “Methods for Engaging and Evaluating Users of Human Computation Systems”, Handbook of Human Computation, pages 679-694, Springer, 2013

[^12]: J. T. Reed, M. J. Raddick, A. Lardner, K. Carney: “An Exploratory Factor Analysis of Motivations for Participating in Zooniverse, a Collection of Virtual Citizen Science Projects”, Proc. 46th Annual Hawaii International Conference on Systems Sciences (HICSS), 2013
[^13]: L. Jian and J. K. MacKie-Mason: “Incentive-Centered Design for User-Contributed Content”, The Oxford Handbook of the Digital Economy, Oxford Handbooks Online, 2012
[^14]: J. Chamberlain, U. Kruschwitz , and M. Poesio: “Methods for Engaging and Evaluating Users of Human Computation Systems”, Handbook of Human Computation, pages 679-694, Springer, 2013
[^15]: J. J. Horton and L. B. Chilton: “The Labor Economics of Paid Crowdsourcing”, Proc. 11th ACM Conference on Electronic Commerce (EC), pages 209–218, 2010

[^16]: W. Mason and D. J. Watts DJ: “Financial incentives and the ‘performance of crowds’ ”, Proc. ACM SIGKDD Workshop on Human Computation, 2009
[^17]: G. Kazai, J. Kamps, and N. Milic-Frayling: “The Face of Quality in Crowdsourcing Relevance Labels: Demographics, Personality and Labeling Accuracy”, Proc. 21st ACM International Conference on Information and Knowledge, pages 2583–2586, 2012
[^18]: J. Chamberlain, U. Kruschwitz , and M. Poesio: “Methods for Engaging and Evaluating Users of Human Computation Systems”, Handbook of Human Computation, pages 679-694, 2013

[^19]: A. Anderson, D. Huttenlocher, J. Kleinberg, J. Leskovec J: “Steering User Behavior With Badges”, Proc. WWW Conference, 2013
[^20]: C. Lintott and J. Reed: “Human Computation in Citizen Science”, Handbook of Human Computation, pages 153-162, 2013

[^21]: J. C. Harsanyi: “Games With Incomplete Information Played by ‘Bayesian’ Players – Part I The Basic Model”, Management Science, 14 (3), pages 159-182, 1967
[^22]: J. C. Harsanyi: “Games With Incomplete Information Played by ‘Bayesian’ Players – Part II Bayesian Equilibirum”, Management Science, 14 (5), pages 320-334, 1968
[^23]: J. C. Harsanyi: “Games With Incomplete Information Played by ‘Bayesian’ Players – Part III The Basic Probability Distribution of the Game”, Management Science, 14 (7), pages 486-502, 1968

[^24]: C. F. Camerer: “Progress in Behavioral Game Theory”, Journal of Economic Perspectives, 11 (4), pages 167-188, 1997
[^25]: C. F. Camerer: “Behavioral Economics: Past, Present, Future”, Chapter in: “Advances in Behavioral Economics”, C. F. Camerer, G. Loewenstein, and M. Rabin, editors, Princeton University Press, 2003
[^26]: D. Ariely: “Predictably Irrational”, Harper Collins Publishers, 2008
[^27]: D. Kahneman: “Thinking Fast and Slow”, FSG (Farrar, Straus and Giroux), 2011

[^28]: N. Alon, F. Fischer, A. Procaccia, M. Tennenholtz: “Sum of us: Strategyproof Selection From the Selectors”, Proc. Conf. Theoretical Aspects of Rationality and Knowledge (TARK), 2011
[^29]: A. Gohsh and R. P. McAfee: “Crowdsourcing With Endogenous Entry”, Proc. 21st ACM International World Wide Web conference (WWW), 2012
[^30]: A. Dasgupta, A. Ghosh: “Crowdsourced Judgment Elicitation With Endogenous proficiency”, Proc. WWW conference, 2013

[^31]: S. Jain S and D. Parkes: “A Game-Theoretic Analysis of the ESP Game”, ACM Transaction on Economics and Computation, 1(1), Article 3, 35 pages, 2013
[^32]: D. Easley and A. Ghosh: “Incentives, Gamification, and Game Theory: An Economic Approach to Badge Design”, Proc. Conf. on Electronic Commerce (EC), 2013
[^33]: A. Anderson, D. Huttenlocher, J. Kleinberg, J. Leskovec: “Steering User Behavior With Badges”, Proc. WWW Conference, 2013
[^34]: A. Ghosh: “Game Theory and Incentives in Human Computation Systems”, Handbook of Human Computation, pages 725-742, 2013